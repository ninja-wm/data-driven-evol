# DDEAs & BBO

<table>

  <tr>
    <th style="font-family: 'Times New Roman', serif;">Title</th>
    <th style="font-family: 'Times New Roman', serif;">Abstract</th>

  </tr>
  <tr>
    <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://openreview.net/pdf?id=vLJcd43U7a">SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning</td>
    <td style="font-family: 'Times New Roman', serif;">
    Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness
    neural networks to meta-learn configurations of traditional black-box optimizers.
    Despite their success, they are inevitably restricted by the limitations of predefined
    hand-crafted optimizers. In this paper, we present SYMBOL, a novel framework
    that promotes the automated discovery of black-box optimizers through symbolic
    equation learning. Specifically, we propose a Symbolic Equation Generator (SEG)
    strategies based on reinforcement learning, so as to meta-learn the SEG efficiently.
    Extensive experiments reveal that the optimizers generated by SYMBOL not only
    surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different
    problem dimensions, population sizes, and optimization horizons. Furthermore,
    we conduct in-depth analyses of our SYMBOL framework and the optimization
    rules that it generates, underscoring its desirable flexibility and interpretability.
    </td>

  </tr>
  <tr>
    <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2304.03995">Discovering Attention-Based Genetic Algorithms via Meta-Black-Box Optimization
    </td>
    <td style="font-family: 'Times New Roman', serif;">
    Genetic algorithms constitute a family of black-box optimization
    algorithms, which take inspiration from the principles of biological
    evolution. While they provide a general-purpose tool for optimization, their particular instantiations can be heuristic and motivated
    by loose biological intuition. In this work we explore a fundamentally different approach: Given a sufficiently flexible parametrization of the genetic operators, we discover entirely new genetic
    algorithms in a data-driven fashion. More specifically, we parametrize selection and mutation rate adaptation as cross- and self-attention modules and use Meta-Black-Box-Optimization to evolve
    their parameters on a set of diverse optimization tasks. The resulting
    Learned Genetic Algorithm outperforms state-of-the-art adaptive baseline genetic algorithms and generalizes far beyond its meta-training settings. The learned algorithm can be applied to previously
    unseen optimization problems, search dimensions & evaluation budgets. We conduct extensive analysis of the discovered operators and
    provide ablation experiments, which highlight the benefits of flexible module parametrization and the ability to transfer (‘plug-in’)
    the learned operators to conventional genetic algorithms.
</tr>
  <tr>
    <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://openreview.net/pdf?id=mFDU0fP3EQH">Discovering Evolution Strategies via Meta-Black-Box Optimization
    <td style="font-family: 'Times New Roman', serif;">Optimizing functions without access to gradients is the remit of black-box methods such as evolution strategies. While highly general, their learning dynamics are
    often times heuristic and inflexible—exactly the limitations that meta-learning
    can address. Hence, we propose to discover effective update rules for evolution
    strategies via meta-learning. Concretely, our approach employs a search strategy
    parametrized by a self-attention-based architecture, which guarantees the update
    rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic
    optimization problems is sufficient to discover new evolution strategies capable of
    generalizing to unseen optimization problems, population sizes and optimization
    horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As
    additional contributions, we ablate the individual neural network components of
    our method; reverse engineer the learned strategy into an explicit heuristic form,
    which remains highly competitive; and show that it is possible to self-referentially
    train an evolution strategy from scratch, with the learned update rule used to drive
    the outer meta-learning loop.

</tr>
  <tr>
    <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2306.07180">Diffusion Models for Black-Box Optimization
    <td style="font-family: 'Times New Roman', serif;">The goal of offline black-box optimization (BBO)
    is to optimize an expensive black-box function using a fixed dataset of function evaluations. Prior
    works consider forward approaches that learn
    surrogates to the black-box function and inverse
    approaches that directly map function values to
    corresponding points in the input domain of the
    black-box function. These approaches are limited
    by the quality of the offline dataset and the difficulty in learning one-to-many mappings in high
    dimensions, respectively. We propose Denoising
    Diffusion Optimization Models (DDOM), a new
    inverse approach for offline black-box optimization based on diffusion models. Given an offline
    dataset, DDOM learns a conditional generative
    model over the domain of the black-box function
    conditioned on the function values. We investigate several design choices in DDOM, such as
    reweighting the dataset to focus on high function values and the use of classifier-free guidance at test-time to enable generalization to function values that can even exceed the dataset maxima. Empirically, we conduct experiments on the
    Design-Bench benchmark (Trabucco et al., 2022)
    and show that DDOM achieves results competitive with state-of-the-art baselines. Our implementation of DDOM can be found at
    https://github.com/siddarthk97/ddom.

</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2103.03526v1">Meta Learning Black-Box Population-Based Optimizers
  <td style="font-family: 'Times New Roman', serif;">The no free lunch theorem states that no model is better suited to every problem. A question that arises from this is how to design methods that propose optimizers tailored to specific problems achieving
  proposing the use of meta-learning to infer population-based black-box optimizers that can automatically adapt to specific classes of
  problems. We suggest a general modeling of population-based algorithms that result in Learning-to-Optimize POMDP (LTO-POMDP),
  a meta-learning framework based on a specific partially observable Markov decision process (POMDP). From that framework’s
  formulation, we propose to parameterize the algorithm using deep
  recurrent neural networks and use a meta-loss function based on
  stochastic algorithms’ performance to train efficient data-driven
  optimizers over several related optimization tasks. The learned optimizers’ performance based on this implementation is assessed on
  various black-box optimization tasks and hyperparameter tuning
  of machine learning models. Our results revealed that the meta-loss
  function encourages a learned algorithm to alter its search behavior
  so that it can easily fit into a new context. Thus, it allows better
  generalization and higher sample efficiency than state-of-the-art
  generic optimization algorithms, such as the Covariance matrix
  adaptation evolution strategy (CMA-ES).

</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2302.12170v2">Language Model Crossover:
  Variation through Few-Shot Prompting
  <td style="font-family: 'Times New Roman', serif;">This paper pursues the insight that language models naturally enable an intelligent variation operator similar in spirit to evolutionary
  crossover. In particular, language models of sufficient scale demonstrate in-context learning, i.e. they can learn from associations
  between a small number of input patterns to generate outputs incorporating such associations (also called few-shot prompting). This
  ability can be leveraged to form a simple but powerful variation operator, i.e. to prompt a language model with a few text-based
  genotypes (such as code, plain-text sentences, or equations), and to parse its corresponding output as those genotypes’ offspring. The
  promise of such language model crossover (which is simple to implement and can leverage many different open-source language
  models) is that it enables a simple mechanism to evolve semantically-rich text representations (with few domain-specific tweaks), and
  naturally benefits from current progress in language models. Experiments in this paper highlight the versatility of language-model crossover, through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code. The conclusion is that
  language model crossover is a flexible and effective method for evolving genomes representable as text.
</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2311.15249">Algorithm Evolution Using Large Language Model
  <td style="font-family: 'Times New Roman', serif;">Optimization can be found in many real-life applications. Designing an effective algorithm for a specific optimization
  problem typically requires a tedious amount of effort from human
  experts with domain knowledge and algorithm design skills.
  In this paper, we propose a novel approach called Algorithm
  Evolution using Large Language Model (AEL). It utilizes a large
  language model (LLM) to automatically generate optimization
  algorithms via an evolutionary framework. AEL does algorithm-level evolution without model training. Human effort and requirements for domain knowledge can be significantly reduced. We
  take constructive methods for the traveling salesman problem as a
  test example, we show that the constructive algorithm obtained
  by AEL outperforms simple hand-crafted and LLM-generated
  heuristics. Compared with other domain deep learning model-based algorithms, these methods exhibit excellent scalability
  across different problem sizes. AEL is also very different from
  previous attempts that utilize LLMs as search operators in
  algorithms.

</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2310.12931">Eureka: Human-Level Reward Design via Coding Large Language Models
  <td style="font-family: 'Times New Roman', serif;">Large Language Models (LLMs) have excelled as high-level semantic planners for
  sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.
  We bridge this fundamental gap and present EUREKA, a human-level reward
  design algorithm powered by LLMs. EUREKA exploits the remarkable zero-shot
  generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code.
  The resulting rewards can then be used to acquire complex skills via reinforcement
  EUREKA generates reward functions that outperform expert human-engineered
  rewards. In a diverse suite of 29 open-source RL environments that include 10
  distinct robot morphologies, EUREKA outperforms human experts on 83% of the
  tasks, leading to an average normalized improvement of 52%. The generality
  of EUREKA also enables a new gradient-free in-context learning approach to
  reinforcement learning from human feedback (RLHF), readily incorporating human
  inputs to improve the quality and the safety of the generated rewards without model
  updating. Finally, using EUREKA rewards in a curriculum learning setting, we
  pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.

</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://papers.nips.cc/paper_files/paper/2023/file/184c1e18d00d7752805324da48ad25be-Paper-Conference.pdf">EvoPrompting: Language Models for Code-Level Neural Architecture Search
  <td style="font-family: 'Times New Roman', serif;">Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as general adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through prompting,
we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EVOPROMPTING, consistently finds diverse and high
performing models. We first demonstrate that EVOPROMPTING is effective on the
computationally efficient MNIST-1D dataset, where EVOPROMPTING produces
convolutional architecture variants that outperform both those designed by human
experts and naive few-shot prompting in terms of accuracy and model size. We then
apply our method to searching for graph neural networks on the CLRS Algorithmic
Reasoning Benchmark, where EVOPROMPTING is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic
reasoning tasks while maintaining similar model size. EVOPROMPTING is successful at designing accurate and efficient neural network architectures across a variety
of machine learning tasks, while also being general enough for easy adaptation to
other tasks beyond neural network design.
</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2306.01102">LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization
  <td style="font-family: 'Times New Roman', serif;">Large language models (LLMs) have emerged as powerful tools
capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made
a significant impact is in the domain of code generation. Here,
we propose using the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile,
Quality-Diversity (QD) algorithms are known to discover diverse
and robust solutions. By merging the code-generating abilities of
LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm.
While LLMs struggle to conduct NAS directly through prompts,
LLMatic uses a procedural approach, leveraging QD for prompts
and network architecture to create diverse and high-performing
networks. We test LLMatic on the CIFAR-10 and NAS-bench-201
benchmarks, demonstrating that it can produce competitive networks while evaluating just 2, 000 candidates, even without prior
knowledge of the benchmark domain or exposure to any previous
top-performing models for the benchmark. The open-sourced code
is available in https://github.com/umair-nasir14/LLMatic.
</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2401.02051">Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model
  <td style="font-family: 'Times New Roman', serif;">Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often
  very labour extensive and requires rich working
  experience and knowledge. This paper proposes
  Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language
  Models (LLMs) and Evolutionary Computation
  (EC) methods for Automatic Heuristic Design
  (AHD). EoH represents the ideas of heuristics in
  natural language, termed thoughts. They are then
  translated into executable codes by LLMs. The
  evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance
  heuristics. Experiments on three widely studied
  combinatorial optimization benchmark problems
  demonstrate that EoH outperforms commonly
  used handcrafted heuristics and other recent AHD
  methods including FunSearch. Particularly, the
  heuristic produced by EoH with a low computational budget (in terms of the number of queries
  human hand-crafted baseline algorithms for the
  online bin packing problem.
</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2406.18851v1">LICO: Large Language Models for In-Context Molecular Optimization
  <td style="font-family: 'Times New Roman', serif;">Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations. Large Language
  Models (LLMs), with their strong pattern-matching capabilities via pretraining on
  vast amounts of data, stand out as a potential candidate for surrogate modeling.
  However, directly prompting a pretrained language model to produce predictions is
  not feasible in many scientific domains due to the scarcity of domain-specific data
  in the pretraining corpora and the challenges of articulating complex problems in
  natural language. In this work, we introduce LICO, a general-purpose model that
  extends arbitrary base LLMs for black-box optimization, with a particular application to the molecular domain. To achieve this, we equip the language model with
  a separate embedding layer and prediction layer, and train the model to perform
  in-context predictions on a diverse set of functions defined over the domain. Once
  trained, LICO can generalize to unseen molecule properties simply via in-context
  prompting. LICO achieves state-of-the-art performance on PMO, a challenging
  molecular optimization benchmark comprising over 20 objective functions.

</tr>
<tr>
  <td style="font-family: 'Times New Roman', serif; font-weight: bold;"><a href="https://arxiv.org/pdf/2403.02131">Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution
  <td style="font-family: 'Times New Roman', serif;">Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However,
the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm
selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of
algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep
reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic
algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm
according to the features observed during the optimization process. To empower the agent with the necessary information, our
framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep
neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context
restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together
enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework
is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase
the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also
demonstrating favorable generalization ability across different problem classes.
</tr>

</table>
